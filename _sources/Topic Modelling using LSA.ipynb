{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7341cf73",
   "metadata": {},
   "source": [
    "# Topic Modelling using LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e3bd6",
   "metadata": {},
   "source": [
    "Algoritma LSA (Latent Semantic Analysis) adalah salah satu algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase/kalimat dengan sekumpulan dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd96963",
   "metadata": {},
   "source": [
    "Pada program ini akan menggunakan data abstrak dari portal tugas akhir trunojoyo program studi Teknik Informatika (https://pta.trunojoyo.ac.id/c_search/byprod/10), berikut code untuk melakukan crawling data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac057508",
   "metadata": {},
   "source": [
    "## Crawling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d82250",
   "metadata": {},
   "source": [
    "Proses pertama yaitu pengambilan data abstrak dari portal tugas akhir menggunakan teknik crawling. Crawling merupakan teknik mengumpulkan data pada sebuah website dengan memasukkan Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638ac88",
   "metadata": {},
   "source": [
    "### Install Library\n",
    "Library yang digunakan adalah beautifulsoap4, jalankan perintah berikut untuk proses instalasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245741b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117298a8",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Selanjutnya import library yang digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8954a4",
   "metadata": {},
   "source": [
    "### Proses Crawling\n",
    "Membuat function crawlAbstract, untuk mengambil data judul dan abstrak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4320008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlAbstract(src):\n",
    "    # inisialisasi beautifulsoup4     \n",
    "    global c\n",
    "    tmp = []\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mengambil data judul     \n",
    "    title = soup.find(class_=\"title\").getText()\n",
    "    tmp.append(title)\n",
    "    \n",
    "    # mengambil data abstract     \n",
    "    abstractText = soup.p.getText()\n",
    "    tmp.append(abstractText)\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85066e",
   "metadata": {},
   "source": [
    "Lalu function getLinkToAbstract berguna untuk mengambil link dari daftar jurnal menuju halaman detail abstrak, function ini akan langsung memanggil crawlAbstract()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5becf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkToAbstract(src):\n",
    "    # inisialisasi beautifulsoup4\n",
    "    global c\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mendapatkan semua link menuju halaman detail\n",
    "    items = soup.find(class_=\"items\").find_all('a')\n",
    "    # looping setiap link untuk mendapatkan nilai href, \n",
    "    # link tersebut digunakan sebagai parameter function crawlAbstract agar mendapat data judul dan abstract\n",
    "    for item in items:\n",
    "        if item.get('href') != '#':\n",
    "            tmp = crawlAbstract(item.get('href'))\n",
    "            # dataAbstract menampung data sementara hasil crawl\n",
    "            dataAbstract.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d1631",
   "metadata": {},
   "source": [
    "Selanjutnya code untuk pemanggilan function, akan dilakukan looping untuk mengurutkan halaman daftar jurnal dari page 1 sampai 100, setiap iterasi akan mengambil link menuju halaman detail abstrak (melalui function getLinkToAbstract()).\n",
    "Looping selanjutnya bertujuan untuk menambahkan id di setiap abstrak hasil crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10\"\n",
    "for i in range(1, 101):\n",
    "    # memindah halaman menuju halaman selanjutnya     \n",
    "    src = f\"https://pta.trunojoyo.ac.id/c_search/byprod/10/{i}\"\n",
    "    # counter untuk melihat progress berapa persen proses crawling\n",
    "    print(f\"Proses-{i}%\")\n",
    "    # memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\n",
    "    getLinkToAbstract(src)\n",
    "\n",
    "# menambahkan id  di setiap abstrak\n",
    "for i in range(1, len(dataAbstract)+1):\n",
    "    dataAbstract[i-1].insert(0, i)\n",
    "    dataFix.append(dataAbstract[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804b17d",
   "metadata": {},
   "source": [
    "### Menyimpan data hasil crawling\n",
    "Semua hasil abstrak akan disimpan format csv dengan nama file dataHasilCrawl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['index', 'title','abstract']\n",
    "with open('dataHasilCrawl.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataFix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a145908",
   "metadata": {},
   "source": [
    "### Code Lengkap Crawling Data\n",
    "Berikut adalah code lengkap proses crawling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# membuat list, dataAbstract untuk menampung data sementara setelah crawling\n",
    "# dataFix untuk menampung data yang sudah ditambahkan kolom index dan siap di convert ke csv\n",
    "dataAbstract = []\n",
    "dataFix = []\n",
    "\n",
    "# function crawlAbstract untuk mengambil data judul dan abstract dari halaman detail pta trunojoyo teknik informatika\n",
    "def crawlAbstract(src):\n",
    "    # inisialisasi beautifulsoup4     \n",
    "    global c\n",
    "    tmp = []\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mengambil data judul     \n",
    "    title = soup.find(class_=\"title\").getText()\n",
    "    tmp.append(title)\n",
    "    \n",
    "    # mengambil data abstract     \n",
    "    abstractText = soup.p.getText()\n",
    "    tmp.append(abstractText)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# function getLinkToAbstract digunakan untuk mengambil data link menuju halaman detail\n",
    "# parameter src berisi link halaman daftar tugas akhir\n",
    "def getLinkToAbstract(src):\n",
    "    # inisialisasi beautifulsoup4\n",
    "    global c\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mendapatkan semua link menuju halaman detail\n",
    "    items = soup.find(class_=\"items\").find_all('a')\n",
    "    # looping setiap link untuk mendapatkan nilai href, \n",
    "    # link tersebut digunakan sebagai parameter function crawlAbstract agar mendapat data judul dan abstract\n",
    "    for item in items:\n",
    "        if item.get('href') != '#':\n",
    "            tmp = crawlAbstract(item.get('href'))\n",
    "            # dataAbstract menampung data sementara hasil crawl\n",
    "            dataAbstract.append(tmp)\n",
    "\n",
    "\n",
    "# link halaman pta trunojoyo prodi teknik informatika yang akan di crawl\n",
    "# halaman ini berisi daftar tugas akhir\n",
    "link = \"https://pta.trunojoyo.ac.id/c_search/byprod/10\"\n",
    "# mengambil data sampai halaman 100\n",
    "for i in range(1, 101):\n",
    "    # memindah halaman menuju halaman selanjutnya     \n",
    "    src = f\"https://pta.trunojoyo.ac.id/c_search/byprod/10/{i}\"\n",
    "    # counter untuk melihat progress berapa persen proses crawling\n",
    "    print(f\"Proses-{i}%\")\n",
    "    # memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\n",
    "    getLinkToAbstract(src)\n",
    "\n",
    "# setelah memperoleh semua data abstract, data tersebut ditampung di list dataAbstract\n",
    "# data perlu ditambahkan kolom index sebagai id\n",
    "# looping berikut bertujuan menambahkan kolom index di setiap baris, lalu disimpan di list dataFix\n",
    "for i in range(1, len(dataAbstract)+1):\n",
    "    dataAbstract[i-1].insert(0, i)\n",
    "    dataFix.append(dataAbstract[i-1])\n",
    "\n",
    "# menyimpan data hasil crawl dengan format csv\n",
    "header = ['index', 'title','abstract']\n",
    "with open('dataHasilCrawl.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataFix)\n",
    "# akan ada file dataHasilCrawl.csv berisi id, judul dan abtrak dari pta trunojoyo teknik informatika sejumlah 500 record\n",
    "# proses crawling selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a5ea1",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c254a0a",
   "metadata": {},
   "source": [
    "Tahap selanjutnya melakukan pre-processing data yang bertujuan agar kualitas data yang digunakan memiliki hasil yang baik dan konsisten. Pre-Processing yang akan dilakukan adalah Case Folding, Punctuation Removal, Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4de96",
   "metadata": {},
   "source": [
    "### Install Library\n",
    "Install terlebih dahulu library yang akan digunakan:\n",
    "Sastrawi digunakan untuk proses stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282ffc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77615974",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Import library dan persiapan, library yang digunakan adalah sastrawi yang digunakan dalam proses stemming dan stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # untuk menyimpan hasil dalam format csv\n",
    "import string \n",
    "import re # re : digunakan untuk proses punctuation removal\n",
    "\n",
    "# memanggil function yang digunakan\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# membuat list untuk menampung data\n",
    "dataAbstract = []\n",
    "dataAfterPreprocessing = []\n",
    "\n",
    "# inisialisasi library sastrawi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# inisialisasi library sastrawi untuk proses stopword removal\n",
    "factory2 = StopWordRemoverFactory()\n",
    "stopword = factory2.create_stop_word_remover()\n",
    "\n",
    "# untuk counter proses\n",
    "count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf32d6",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Selanjutnya dilakukan proses data load dari file dataHasilCrawl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataHasilCrawl.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        if len(row) != 0:\n",
    "#           data sebelum proses disimpan pada list dataAbstract\n",
    "            dataAbstract.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4996f",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Akan dilakukan pre-processing yang meliputi:\n",
    "1. Case Folding\n",
    "Case folding merupakan proses dalam text preprocessing yang dilakukan untuk menyeragamkan karakter pada data. Proses case folding adalah proses mengubah seluruh huruf menjadi huruf kecil. Pada proses ini karakter-karakter 'A'-'Z' yang terdapat pada data diubah kedalam karakter 'a'-'z'\n",
    "\n",
    "2. Punctuation Removal\n",
    "Punctuation Removal adalah proses menghilangkan tanda baca, simbol, angka dan spasi yang tidak perlu dalam dataset.\n",
    "\n",
    "3. Stemming\n",
    "Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Secara sederhana, proses mengubah kata berimbuhan menjadi kata dasar.\n",
    "\n",
    "4. Stopwords\n",
    "Stopwords adalah kata yang diabaikan dalam pemrosesan karena merupakan kata umum yang mempunyai fungsi tapi tidak mempunyai arti.\n",
    "\n",
    "Berikut adalah code untuk melakukan pre-processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b93bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in dataAbstract:\n",
    "#   ambil data\n",
    "    tmp = abstract.pop()\n",
    "#   lakukan case folding (mengubah teks menjadi bentuk standar: huruf kecil)\n",
    "    tmp = tmp.lower()\n",
    "#   menghapus angka\n",
    "    tmp = re.sub(r\"\\d+\", \"\", tmp)\n",
    "#   menghapus tanda baca\n",
    "    tmp = tmp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "#   menghapus whitespace\n",
    "    tmp = tmp.strip()\n",
    "    tmp = re.sub('\\s+',' ',tmp)\n",
    "#   melakukan proses stemming\n",
    "#     tmp = stemmer.stem(tmp)\n",
    "#   melakukan proses stopword removal\n",
    "    tmp = stopword.remove(tmp)\n",
    "#   menambahkan data ke list dataAfterPreprocessing\n",
    "    abstract.append(tmp)\n",
    "    dataAfterPreprocessing.append(abstract)\n",
    "#   print counter proses\n",
    "    print(f\"Proses:{count}/{len(dataAbstract)}\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70471dda",
   "metadata": {},
   "source": [
    "### Menyimpan data hasil Pre-Processing\n",
    "data hasil preprocessing disimpan dalam bentuk csv dengan nama file dataAfterPreprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a752e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan data dari list dataAfterPreprocessing ke bentuk csv\n",
    "header = ['index', 'title','abstract_cleaned']\n",
    "with open('dataAfterPreprocessing.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataAfterPreprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d00d6",
   "metadata": {},
   "source": [
    "### Code lengkap Pre-Processing\n",
    "Berikut adalah code lengkap Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # untuk menyimpan hasil dalam format csv\n",
    "import string \n",
    "import re # re : digunakan untuk proses punctuation removal\n",
    "\n",
    "# memanggil function yang digunakan\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# membuat list untuk menampung data\n",
    "dataAbstract = []\n",
    "dataAfterPreprocessing = []\n",
    "\n",
    "# inisialisasi library sastrawi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# inisialisasi library sastrawi untuk proses stopword removal\n",
    "factory2 = StopWordRemoverFactory()\n",
    "stopword = factory2.create_stop_word_remover()\n",
    "\n",
    "# untuk counter proses\n",
    "count = 1\n",
    "\n",
    "# membaca data dari proses sebelumnya\n",
    "with open(\"dataHasilCrawl.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        if len(row) != 0:\n",
    "#           data sebelum proses disimpan pada list dataAbstract\n",
    "            dataAbstract.append(row)\n",
    "\n",
    "# looping untuk memproses setiap data\n",
    "for abstract in dataAbstract:\n",
    "#   ambil data\n",
    "    tmp = abstract.pop()\n",
    "#   lakukan case folding (mengubah teks menjadi bentuk standar: huruf kecil)\n",
    "    tmp = tmp.lower()\n",
    "#   menghapus angka\n",
    "    tmp = re.sub(r\"\\d+\", \"\", tmp)\n",
    "#   menghapus tanda baca\n",
    "    tmp = tmp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "#   menghapus whitespace\n",
    "    tmp = tmp.strip()\n",
    "    tmp = re.sub('\\s+',' ',tmp)\n",
    "#   melakukan proses stemming\n",
    "#     tmp = stemmer.stem(tmp)\n",
    "#   melakukan proses stopword removal\n",
    "    tmp = stopword.remove(tmp)\n",
    "#   menambahkan data ke list dataAfterPreprocessing\n",
    "    abstract.append(tmp)\n",
    "    dataAfterPreprocessing.append(abstract)\n",
    "#   print counter proses\n",
    "    print(f\"Proses:{count}/{len(dataAbstract)}\")\n",
    "    count+=1\n",
    "\n",
    "# menyimpan data dari list dataAfterPreprocessing ke bentuk csv\n",
    "header = ['index', 'title','abstract_cleaned']\n",
    "with open('dataAfterPreprocessing.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataAfterPreprocessing)\n",
    "# akan ada file dataAfterPreprocessing.csv berisi id, judul, abtract yang sudah dipreprocessing\n",
    "# preprocessing sudah selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86675ebe",
   "metadata": {},
   "source": [
    "## Permodelan dengan LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9da9b3",
   "metadata": {},
   "source": [
    "Masuk ke tahap penerapan Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ba305",
   "metadata": {},
   "source": [
    "### Install Library\n",
    "install library yang akan digunakan yaitu sklearn, pandas, matplotlib dan seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43287dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d553a",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Berikut adalah proses import library dan inisialisasi library sebelum digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47fe448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi semua library yg digunakan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# mengatur tampilan matplotlib ketika menampilkan data\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cd3ac1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# menggunakan library sklearn untuk membuat tfidf, disini baru import function-nya dulu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a26a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords  #stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73633b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop_words=set(nltk.corpus.stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b81fe",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Berikut adalah code untuk membaca data dari dataAfterPreprocessing.csv, karena yang digunakan hanya kolom abstrak, maka kolom id dan title dihapus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45547a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PERANCANGAN DAN IMPLEMENTASI SISTEM DATABASE \\...</td>\n",
       "      <td>sistem informasi akademik siakad merupakan sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>APLIKASI KONTROL DAN MONITORING JARINGAN KOMPU...</td>\n",
       "      <td>berjalannya koneksi jaringan komputer lancar g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RANCANG BANGUN APLIKASI PROXY SERVER UNTUK\\r\\n...</td>\n",
       "      <td>web server sebuah perangkat lunak server berfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SISTEM PENDUKUNG KEPUTUSAN OPTIMASI PENJADWALA...</td>\n",
       "      <td>penjadwalan kuliah perguruan tinggi merupakan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SISTEM AUGMENTED REALITY ANIMASI BENDA BERGERA...</td>\n",
       "      <td>seiring perkembangan teknologi ada didunia mun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  \\\n",
       "0      1  PERANCANGAN DAN IMPLEMENTASI SISTEM DATABASE \\...   \n",
       "1      2  APLIKASI KONTROL DAN MONITORING JARINGAN KOMPU...   \n",
       "2      3  RANCANG BANGUN APLIKASI PROXY SERVER UNTUK\\r\\n...   \n",
       "3      4  SISTEM PENDUKUNG KEPUTUSAN OPTIMASI PENJADWALA...   \n",
       "4      5  SISTEM AUGMENTED REALITY ANIMASI BENDA BERGERA...   \n",
       "\n",
       "                                    abstract_cleaned  \n",
       "0  sistem informasi akademik siakad merupakan sis...  \n",
       "1  berjalannya koneksi jaringan komputer lancar g...  \n",
       "2  web server sebuah perangkat lunak server berfu...  \n",
       "3  penjadwalan kuliah perguruan tinggi merupakan ...  \n",
       "4  seiring perkembangan teknologi ada didunia mun...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# membaca data\n",
    "df=pd.read_csv('./dataAfterPreprocessing.csv')\n",
    "# menampilkan data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3235af",
   "metadata": {},
   "source": [
    "Menghapus kolom id dan title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeeb7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus data index dan title karena tidak digunakan\n",
    "df.drop(['index'],axis=1,inplace=True)\n",
    "df.drop(['title'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9862a8",
   "metadata": {},
   "source": [
    "Data abstract siap digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35a46568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sistem informasi akademik siakad merupakan sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berjalannya koneksi jaringan komputer lancar g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>web server sebuah perangkat lunak server berfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penjadwalan kuliah perguruan tinggi merupakan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seiring perkembangan teknologi ada didunia mun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gerak pekerja game memiliki genre rts realtime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perkembangan game semakin pesat memberikan ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sistem pengenalan wajah suatu sistem mengenali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teknologi mobile game beroperating system open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kantor badan kepegawaian kota bangkalan instan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    abstract_cleaned\n",
       "0  sistem informasi akademik siakad merupakan sis...\n",
       "1  berjalannya koneksi jaringan komputer lancar g...\n",
       "2  web server sebuah perangkat lunak server berfu...\n",
       "3  penjadwalan kuliah perguruan tinggi merupakan ...\n",
       "4  seiring perkembangan teknologi ada didunia mun...\n",
       "5  gerak pekerja game memiliki genre rts realtime...\n",
       "6  perkembangan game semakin pesat memberikan ber...\n",
       "7  sistem pengenalan wajah suatu sistem mengenali...\n",
       "8  teknologi mobile game beroperating system open...\n",
       "9  kantor badan kepegawaian kota bangkalan instan..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan 10 baris data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b0084",
   "metadata": {},
   "source": [
    "### Extracting Feature dan Membuat Document Term-Matrix (DTM)\n",
    "Nilai DTM menggunakan nilai TF-Idf.\n",
    "Beberapa poin penting yang perlu diperhatikan:\n",
    "1. LSA pada umumnya diimplementasikan dengan menggunakan nilai TF-Idf dan tidak dengan Count Vectorizer.\n",
    "2. Nilai parameter max_feature bergantung pada daya komputasi.\n",
    "3. Nilai default untuk min_df dan max_df agar program dapat bekerja dengan baik.\n",
    "4. Bisa menggunakan nilai ngram_range yang berbeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c9b5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menghitung tfidf\n",
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(df['abstract_cleaned'].values.astype('U'))\n",
    "type(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c315143",
   "metadata": {},
   "source": [
    "Dapat dilihat pada hasilnya, kata yang sering muncul dan jarang muncul dalam abstrak yang ada dalam idf. Apabila hasil memiliki nilai yang kecil maka kata tersebut lebih umum digunakan dalam dokumen (abstrak PTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835f521",
   "metadata": {},
   "source": [
    "### Document Term Matrix (DTM)\n",
    "Setiap baris mewakili sebuah kata yang unik, sedangkan setiap kolom mewakili konteks dari mana kata-kata tersebut diambil. Konteks yang dimaksud bisa berupa kalimat, paragraf, atau seluruh bagian dari teks.\n",
    "Berikut adalah term-document matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae0f35",
   "metadata": {},
   "source": [
    "![Term Document Matrix](termDocumentMatrix.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a269b6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500)\n",
      "   0    1    2    3    4    5    6    7         8    9    ...  990  991  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.327224  0.0  ...  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0   \n",
      "\n",
      "        992  993  994  995  996  997  998  999  \n",
      "0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.392931  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "# print(vect_text)\n",
    "type(vect_text)\n",
    "vect_text = vect_text.transpose()\n",
    "df = pd.DataFrame(vect_text.toarray())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a229dac",
   "metadata": {},
   "source": [
    "Kita sekarang dapat melihat kata-kata yang paling sering dan langka di abstrak berdasarkan skor idf. Semakin kecil nilainya berarti kata tersebut lebih sering digunakan (umum) dalam abstrak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "810ccedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil telapak\n",
      "1.3701673260271405\n",
      "6.523458920524919\n"
     ]
    }
   ],
   "source": [
    "idf=vect.idf_\n",
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['hasil'])\n",
    "print(dd['telapak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe936a72",
   "metadata": {},
   "source": [
    "Dapat dilihat kata paling sering digunakan adalah \"hasil\" sementara kata paling jarang digunakan adalah \"telapak\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7571361",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "LSA pada dasarnya adalah dekomposisi dari nilai tunggal.\n",
    "Singular Value Decomposition (SVD) akan menguraikan DTM menjadi tiga matriks: \n",
    "\n",
    "$A_{m n}=U_{m m} x S_{m n} x V_{n n}^{T}$\n",
    "\n",
    "\n",
    "Matriks U = Pada matriks ini, baris mewakili vektor dokumen pada topik\n",
    "Matriks V = Baris pada matriks ini mewakili vektor istilah yang dinyatakan pada topik\n",
    "Matriks S = Matriks diagonal yang memiliki elemen-elemen diagonal sebagai nilai singular dari A\n",
    "\n",
    "Pada setiap baris dari matriks U (matriks istilah dari dokumen) merupakan representasi vektor yang ada dalam dokumen yang sesuai. Panjang vektor ini ialah jumlah topik yang diinginkan. Representasi dari vektor untuk suku yang ada dalam data dapat ditemui dalam matriks V.\n",
    "\n",
    "Jadi, SVD memberikan nilai vektor pada setiap dokumen dan juga istilah dalam data. Panjang dari setiap vektor adalah k. Vektor ini digunakan untuk menentukan kata dan dokumen serupa dalam metode kesamaan kosinus.\n",
    "\n",
    "Dapat digunakan fungsi truncastedSVD untuk mengimplementasikan LSA. Parameter n_components merupakan jumlah topik yang akan diekstrak. Model tersebut nantinya akan di fit dan ditransformasikan pada hasil yang diberikan oleh vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dc3f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e9dbc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25314743 -0.07926497 -0.15779302 ...  0.07078405 -0.15720191\n",
      "   0.08248597]\n",
      " [ 0.08594745 -0.03921934 -0.01889975 ... -0.0277596  -0.07017209\n",
      "  -0.03474838]\n",
      " [ 0.10875014 -0.05476071 -0.01671957 ...  0.27305754 -0.01860027\n",
      "  -0.00717952]\n",
      " ...\n",
      " [ 0.21358561 -0.01412773 -0.09962852 ... -0.23532446  0.11006678\n",
      "   0.23077264]\n",
      " [ 0.28543252 -0.044966   -0.11713009 ... -0.01439869  0.01721985\n",
      "  -0.06424741]\n",
      " [ 0.19844412 -0.07808743  0.01047994 ...  0.01968828  0.01092608\n",
      "  -0.10894619]]\n",
      "(500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)  # (no_of_doc*no_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da02a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  25.314742581593652\n",
      "Topic  1  :  -7.92649675944443\n",
      "Topic  2  :  -15.779302240752369\n",
      "Topic  3  :  -14.716978564412589\n",
      "Topic  4  :  3.1661880386549996\n",
      "Topic  5  :  -10.053559857839996\n",
      "Topic  6  :  5.665605564559997\n",
      "Topic  7  :  7.078404802169859\n",
      "Topic  8  :  -15.720191082463902\n",
      "Topic  9  :  8.248596705601583\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "    print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bde57663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "[[ 0.01990259  0.00868318  0.01116126 ...  0.0112083   0.00893897\n",
      "   0.01253246]\n",
      " [-0.01279456  0.00508693  0.00048887 ... -0.00664785 -0.0040516\n",
      "  -0.00555939]\n",
      " [-0.00221451 -0.00221382 -0.00424869 ... -0.00809025 -0.00825992\n",
      "  -0.01063404]\n",
      " ...\n",
      " [-0.00339135  0.00548564  0.00708094 ...  0.01098567 -0.01600809\n",
      "  -0.02055816]\n",
      " [ 0.00294209  0.0022143   0.00645995 ...  0.01137881  0.0334048\n",
      "   0.05155663]\n",
      " [-0.0035613  -0.00520553 -0.00259026 ...  0.00077315 -0.03029028\n",
      "  -0.04712787]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9fa72",
   "metadata": {},
   "source": [
    "### Hasil\n",
    "Berikut adalah 10 kata penting dalam setiap topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "929e9486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "citra sistem data metode game siswa nilai hasil informasi proses \n",
      "\n",
      "Topic 1: \n",
      "citra segmentasi fitur tahap wajah darah penyakit ekstraksi pembuluh akurasi \n",
      "\n",
      "Topic 2: \n",
      "game citra edukasi pembelajaran anak android permainan menarik matematika bahasa \n",
      "\n",
      "Topic 3: \n",
      "siswa keputusan kriteria pendukung beasiswa segmentasi game citra metode saw \n",
      "\n",
      "Topic 4: \n",
      "siswa pembelajaran elearning guru belajar sekolah media aplikasi materi bahasa \n",
      "\n",
      "Topic 5: \n",
      "wajah pengenalan siswa distance ekspresi analysis euclidean tangan pembelajaran training \n",
      "\n",
      "Topic 6: \n",
      "arsitektur perusahaan erp sistem zf driven penilaian enterprise model wajah \n",
      "\n",
      "Topic 7: \n",
      "beasiswa dokumen pencarian web bahasa informasi keputusan madura semantik sistem \n",
      "\n",
      "Topic 8: \n",
      "madura bahasa batik pembelajaran elearning belajar model indonesia perusahaan mahasiswa \n",
      "\n",
      "Topic 9: \n",
      "batik citra siswa warna pencarian sekolah tekstur isi game akademik \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcebf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
