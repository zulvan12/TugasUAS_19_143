{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aba186",
   "metadata": {},
   "source": [
    "# Analisa Topik Modelling menggunakan Metode Latent Semantic Analysis  dan Analisa K-Mean Clustering dengan Data Abstrak Ekonomi-Manajemen di pta.trunojoyo.ac.id "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9562d1",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e0ee4",
   "metadata": {},
   "source": [
    "Proses pertama yaitu pengambilan data abstrak Ekonomi Manajemen dari Portal Tugas Akhir Trunojoyo menggunakan teknik crawling. Crawling merupakan teknik mengumpulkan data pada sebuah website dengan memasukkan Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72445bcb",
   "metadata": {},
   "source": [
    "### Install Library\n",
    "Langkah pertama adalah melakukan instalasi Library yang digunakan yaitu beautifulsoap4, jalankan perintah berikut untuk proses instalasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bb1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo-pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo-pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\lenovo-pc\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de722e98",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Selanjutnya import library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fe6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97fb52",
   "metadata": {},
   "source": [
    "### Proses Crawling\n",
    "Membuat function crawlAbstract, untuk mengambil data judul dan abstrak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ec4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function crawlAbstract untuk mengambil data judul dan abstract dari halaman detail pta trunojoyo teknik informatika\n",
    "def crawlAbstract(src):\n",
    "    # inisialisasi beautifulsoup4     \n",
    "    global c\n",
    "    tmp = []\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mengambil data judul     \n",
    "    title = soup.find(class_=\"title\").getText()\n",
    "    tmp.append(title)\n",
    "    \n",
    "    # mengambil data abstract     \n",
    "    abstractText = soup.p.getText()\n",
    "    tmp.append(abstractText)\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712a08b",
   "metadata": {},
   "source": [
    "Lalu function getLinkToAbstract berguna untuk mengambil link dari daftar jurnal menuju halaman detail abstrak, function ini akan langsung memanggil crawlAbstract()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74224a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkToAbstract(src):\n",
    "    # inisialisasi beautifulsoup4\n",
    "    global c\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     print(src)\n",
    "    \n",
    "    # mendapatkan semua link menuju halaman detail\n",
    "    items = soup.find(class_=\"items\").find_all('a')\n",
    "    # looping setiap link untuk mendapatkan nilai href, \n",
    "    # link tersebut digunakan sebagai parameter function crawlAbstract agar mendapat data judul dan abstract\n",
    "    for item in items:\n",
    "        if item.get('href') != '#':\n",
    "#             print(item)\n",
    "            tmp = crawlAbstract(item.get('href'))\n",
    "            # dataAbstract menampung data sementara hasil crawl\n",
    "            dataAbstract.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d29d5",
   "metadata": {},
   "source": [
    "Selanjutnya code untuk pemanggilan function, akan dilakukan looping untuk mengurutkan halaman daftar jurnal dari page 1 sampai terakhir, setiap iterasi akan mengambil link menuju halaman detail abstrak (melalui function getLinkToAbstract()). Looping selanjutnya bertujuan untuk menambahkan id di setiap abstrak hasil crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8144f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses-0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataAbstract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProses-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mmaxPage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mgetLinkToAbstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# setelah memperoleh semua data abstract, data tersebut ditampung di list dataAbstract\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# data perlu ditambahkan kolom index sebagai id\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# looping berikut bertujuan menambahkan kolom index di setiap baris, lalu disimpan di list dataFix\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataAbstract)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mgetLinkToAbstract\u001b[1;34m(src)\u001b[0m\n\u001b[0;32m     15\u001b[0m tmp \u001b[38;5;241m=\u001b[39m crawlAbstract(item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# dataAbstract menampung data sementara hasil crawl\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdataAbstract\u001b[49m\u001b[38;5;241m.\u001b[39mappend(tmp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataAbstract' is not defined"
     ]
    }
   ],
   "source": [
    "global c\n",
    "page = requests.get(\"https://pta.trunojoyo.ac.id/c_search/byprod/7\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "maxPage = soup.find_all(class_=\"pag_button\")\n",
    "maxPage = maxPage[4]\n",
    "maxPage = maxPage.get('href')\n",
    "maxPage = maxPage[-3:]\n",
    "maxPage = int(maxPage)\n",
    "\n",
    "for i in range(1, maxPage+1):\n",
    "    # memindah halaman menuju halaman selanjutnya     \n",
    "    src = f\"https://pta.trunojoyo.ac.id/c_search/byprod/7/{i}\"\n",
    "    # counter untuk melihat progress berapa persen proses crawling\n",
    "    print(f\"Proses-{i//maxPage}%\")\n",
    "    # memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\n",
    "    getLinkToAbstract(src)\n",
    "\n",
    "# setelah memperoleh semua data abstract, data tersebut ditampung di list dataAbstract\n",
    "# data perlu ditambahkan kolom index sebagai id\n",
    "# looping berikut bertujuan menambahkan kolom index di setiap baris, lalu disimpan di list dataFix\n",
    "for i in range(1, len(dataAbstract)+1):\n",
    "    dataAbstract[i-1].insert(0, i)\n",
    "    dataFix.append(dataAbstract[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88335eba",
   "metadata": {},
   "source": [
    "### Menyimpan data hasil crawling\n",
    "Semua hasil abstrak akan disimpan format csv dengan nama file dataHasilCrawl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan data hasil crawl dengan format csv\n",
    "header = ['index', 'title','abstract']\n",
    "with open('dataHasilCrawl.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataFix)\n",
    "# akan ada file dataHasilCrawl.csv berisi id, judul dan abtrak dari pta trunojoyo teknik informatika sejumlah 500 record\n",
    "# proses crawling selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2bc32",
   "metadata": {},
   "source": [
    "### Code Lengkap Crawling Data\n",
    "Berikut adalah code lengkap proses crawling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff76c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# membuat list, dataAbstract untuk menampung data sementara setelah crawling\n",
    "# dataFix untuk menampung data yang sudah ditambahkan kolom index dan siap di convert ke csv\n",
    "dataAbstract = []\n",
    "dataFix = []\n",
    "\n",
    "# function crawlAbstract untuk mengambil data judul dan abstract dari halaman detail pta trunojoyo teknik informatika\n",
    "def crawlAbstract(src):\n",
    "    # inisialisasi beautifulsoup4     \n",
    "    global c\n",
    "    tmp = []\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # mengambil data judul     \n",
    "    title = soup.find(class_=\"title\").getText()\n",
    "    tmp.append(title)\n",
    "    \n",
    "    # mengambil data abstract     \n",
    "    abstractText = soup.p.getText()\n",
    "    tmp.append(abstractText)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# function getLinkToAbstract digunakan untuk mengambil data link menuju halaman detail\n",
    "# parameter src berisi link halaman daftar tugas akhir\n",
    "def getLinkToAbstract(src):\n",
    "    # inisialisasi beautifulsoup4\n",
    "    global c\n",
    "    page = requests.get(src)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     print(src)\n",
    "    \n",
    "    # mendapatkan semua link menuju halaman detail\n",
    "    items = soup.find(class_=\"items\").find_all('a')\n",
    "    # looping setiap link untuk mendapatkan nilai href, \n",
    "    # link tersebut digunakan sebagai parameter function crawlAbstract agar mendapat data judul dan abstract\n",
    "    for item in items:\n",
    "        if item.get('href') != '#':\n",
    "#             print(item)\n",
    "            tmp = crawlAbstract(item.get('href'))\n",
    "            # dataAbstract menampung data sementara hasil crawl\n",
    "            dataAbstract.append(tmp)\n",
    "\n",
    "\n",
    "# link halaman pta trunojoyo prodi teknik informatika yang akan di crawl\n",
    "# halaman ini berisi daftar tugas akhir\n",
    "# link = \"https://pta.trunojoyo.ac.id/c_search/byprod/7\"\n",
    "# mengambil data sampai halaman terakhir\n",
    "global c\n",
    "page = requests.get(\"https://pta.trunojoyo.ac.id/c_search/byprod/7\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "maxPage = soup.find_all(class_=\"pag_button\")\n",
    "maxPage = maxPage[4]\n",
    "maxPage = maxPage.get('href')\n",
    "maxPage = maxPage[-3:]\n",
    "maxPage = int(maxPage)\n",
    "\n",
    "for i in range(1, maxPage+1):\n",
    "    # memindah halaman menuju halaman selanjutnya     \n",
    "    src = f\"https://pta.trunojoyo.ac.id/c_search/byprod/7/{i}\"\n",
    "    # counter untuk melihat progress berapa persen proses crawling\n",
    "    print(f\"Proses-{i//maxPage}%\")\n",
    "    # memanggil function getLinkToAbstract untuk mendapatkan setiap link ke halaman detail\n",
    "    getLinkToAbstract(src)\n",
    "\n",
    "# setelah memperoleh semua data abstract, data tersebut ditampung di list dataAbstract\n",
    "# data perlu ditambahkan kolom index sebagai id\n",
    "# looping berikut bertujuan menambahkan kolom index di setiap baris, lalu disimpan di list dataFix\n",
    "for i in range(1, len(dataAbstract)+1):\n",
    "    dataAbstract[i-1].insert(0, i)\n",
    "    dataFix.append(dataAbstract[i-1])\n",
    "\n",
    "# menyimpan data hasil crawl dengan format csv\n",
    "header = ['index', 'title','abstract']\n",
    "with open('dataHasilCrawl.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataFix)\n",
    "# akan ada file dataHasilCrawl.csv berisi id, judul dan abtrak dari pta trunojoyo teknik informatika sejumlah 500 record\n",
    "# proses crawling selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ad4a8",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Tahap selanjutnya melakukan pre-processing data yang bertujuan agar kualitas data yang digunakan memiliki hasil yang baik dan konsisten. Pre-Processing yang akan dilakukan adalah Case Folding, Punctuation Removal, Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dda093",
   "metadata": {},
   "source": [
    "### Install Library\n",
    "Install terlebih dahulu library yang akan digunakan: Sastrawi digunakan untuk proses stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sastrawi\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34ad46",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Import library dan persiapan, library yang digunakan adalah sastrawi yang digunakan dalam proses stemming dan stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # untuk menyimpan hasil dalam format csv\n",
    "import string \n",
    "import re # re : digunakan untuk proses punctuation removal\n",
    "\n",
    "# memanggil function yang digunakan\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# membuat list untuk menampung data\n",
    "dataAbstract = []\n",
    "dataAfterPreprocessing = []\n",
    "\n",
    "# inisialisasi library sastrawi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# inisialisasi library sastrawi untuk proses stopword removal\n",
    "factory2 = StopWordRemoverFactory()\n",
    "stopword = factory2.create_stop_word_remover()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# untuk counter proses\n",
    "count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e42be",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Selanjutnya dilakukan proses data load dari file dataHasilCrawl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataHasilCrawl.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        if len(row) != 0:\n",
    "#           data sebelum proses disimpan pada list dataAbstract\n",
    "            dataAbstract.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdedd0",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Akan dilakukan pre-processing yang meliputi:\n",
    "1. Case Folding\n",
    "Case folding merupakan proses dalam text preprocessing yang dilakukan untuk menyeragamkan karakter pada data. Proses case folding adalah proses mengubah seluruh huruf menjadi huruf kecil. Pada proses ini karakter-karakter 'A'-'Z' yang terdapat pada data diubah kedalam karakter 'a'-'z'\n",
    "\n",
    "2. Punctuation Removal\n",
    "Punctuation Removal adalah proses menghilangkan tanda baca, simbol, angka dan spasi yang tidak perlu dalam dataset.\n",
    "\n",
    "3. Stemming\n",
    "Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Secara sederhana, proses mengubah kata berimbuhan menjadi kata dasar.\n",
    "\n",
    "4. Stopwords\n",
    "Stopwords adalah kata yang diabaikan dalam pemrosesan karena merupakan kata umum yang mempunyai fungsi tapi tidak mempunyai arti.\n",
    "\n",
    "Berikut adalah code untuk melakukan pre-processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping untuk memproses setiap data\n",
    "for abstract in dataAbstract:\n",
    "#   ambil data\n",
    "    tmp = abstract.pop()\n",
    "#   lakukan case folding (mengubah teks menjadi bentuk standar: huruf kecil)\n",
    "    tmp = tmp.lower()\n",
    "#   menghapus angka\n",
    "    tmp = re.sub(r\"\\d+\", \"\", tmp)\n",
    "#   menghapus tanda baca\n",
    "    tmp = tmp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "#   menghapus whitespace\n",
    "    tmp = tmp.strip()\n",
    "    tmp = re.sub('\\s+',' ',tmp)\n",
    "#   melakukan proses stemming\n",
    "#     tmp = stemmer.stem(tmp)\n",
    "\n",
    "    tokens = word_tokenize(tmp)\n",
    "    listStopword =  set(stopwords.words('indonesian'))\n",
    " \n",
    "    removed = []\n",
    "    for t in tokens:\n",
    "        if t not in listStopword:\n",
    "            removed.append(t)\n",
    "    \n",
    "    removed = ' '.join(removed)\n",
    "    abstract.append(removed)\n",
    "    dataAfterPreprocessing.append(abstract)\n",
    "#   print counter proses\n",
    "    print(f\"Proses:{count}/{len(dataAbstract)}\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d306862",
   "metadata": {},
   "source": [
    "### Menyimpan data hasil Pre-Processing\n",
    "data hasil preprocessing disimpan dalam bentuk csv dengan nama file dataAfterPreprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1740113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan data dari list dataAfterPreprocessing ke bentuk csv\n",
    "header = ['index', 'title','abstract_cleaned']\n",
    "with open('dataAfterPreprocessing.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataAfterPreprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d28c",
   "metadata": {},
   "source": [
    "### Code lengkap Pre-Processing\n",
    "Berikut adalah code lengkap Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # untuk menyimpan hasil dalam format csv\n",
    "import string \n",
    "import re # re : digunakan untuk proses punctuation removal\n",
    "\n",
    "# memanggil function yang digunakan\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# membuat list untuk menampung data\n",
    "dataAbstract = []\n",
    "dataAfterPreprocessing = []\n",
    "\n",
    "# inisialisasi library sastrawi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# inisialisasi library sastrawi untuk proses stopword removal\n",
    "factory2 = StopWordRemoverFactory()\n",
    "stopword = factory2.create_stop_word_remover()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# untuk counter proses\n",
    "count = 1\n",
    "\n",
    "# membaca data dari proses sebelumnya\n",
    "with open(\"dataHasilCrawl.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        if len(row) != 0:\n",
    "#           data sebelum proses disimpan pada list dataAbstract\n",
    "            dataAbstract.append(row)\n",
    "\n",
    "# looping untuk memproses setiap data\n",
    "for abstract in dataAbstract:\n",
    "#   ambil data\n",
    "    tmp = abstract.pop()\n",
    "#   lakukan case folding (mengubah teks menjadi bentuk standar: huruf kecil)\n",
    "    tmp = tmp.lower()\n",
    "#   menghapus angka\n",
    "    tmp = re.sub(r\"\\d+\", \"\", tmp)\n",
    "#   menghapus tanda baca\n",
    "    tmp = tmp.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "#   menghapus whitespace\n",
    "    tmp = tmp.strip()\n",
    "    tmp = re.sub('\\s+',' ',tmp)\n",
    "#   melakukan proses stemming\n",
    "#     tmp = stemmer.stem(tmp)\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(tmp)\n",
    "    listStopword =  set(stopwords.words('indonesian'))\n",
    " \n",
    "    removed = []\n",
    "    for t in tokens:\n",
    "        if t not in listStopword:\n",
    "            removed.append(t)\n",
    "    \n",
    "    removed = ' '.join(removed)\n",
    "    abstract.append(removed)\n",
    "    dataAfterPreprocessing.append(abstract)\n",
    "#   print counter proses\n",
    "    print(f\"Proses:{count}/{len(dataAbstract)}\")\n",
    "    count+=1\n",
    "\n",
    "# menyimpan data dari list dataAfterPreprocessing ke bentuk csv\n",
    "header = ['index', 'title','abstract_cleaned']\n",
    "with open('dataAfterPreprocessing.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(header)\n",
    "    write.writerows(dataAfterPreprocessing)\n",
    "# akan ada file dataAfterPreprocessing.csv berisi id, judul, abtract yang sudah dipreprocessing\n",
    "# preprocessing sudah selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47eee5a",
   "metadata": {},
   "source": [
    "## Pemodelan dengan LSA\n",
    "Masuk ke tahap penerapan Latent Semantic Analysis (LSA)\n",
    "\n",
    "### Install Library\n",
    "install library yang akan digunakan yaitu sklearn, pandas, matplotlib dan seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bebb5d",
   "metadata": {},
   "source": [
    "### Import Library\n",
    "Berikut adalah proses import library dan inisialisasi library sebelum digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3402e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi semua library yg digunakan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# mengatur tampilan matplotlib ketika menampilkan data\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7c0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menggunakan library sklearn untuk membuat tfidf, disini baru import function-nya dulu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  #stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc18976",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(nltk.corpus.stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516fc46",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Berikut adalah code untuk membaca data dari dataAfterPreprocessing.csv, karena yang digunakan hanya kolom abstrak, maka kolom id dan title dihapus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce02c005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PENGARUH FAKTOR-FAKTOR PELATIHAN DAN PENGEMBAN...</td>\n",
       "      <td>abstrak satiyah pengaruh faktorfaktor pelatiha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ANALISIS PERSEPSI BRAND ASSOCIATION MENURUT PE...</td>\n",
       "      <td>tujuan penelitian persepsi brand association p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PENGARUH GAYA KEPEMIMPINAN DEMOKRATIK TERHADAP...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pengukuran Website Quality Pada Situs Sistem A...</td>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PENGARUH KEPEMIMPINAN DAN KOMPENSASI TERHADAP ...</td>\n",
       "      <td>abstrak penelitian metode kuantitatif menekank...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  \\\n",
       "0      1  PENGARUH FAKTOR-FAKTOR PELATIHAN DAN PENGEMBAN...   \n",
       "1      2  ANALISIS PERSEPSI BRAND ASSOCIATION MENURUT PE...   \n",
       "2      3  PENGARUH GAYA KEPEMIMPINAN DEMOKRATIK TERHADAP...   \n",
       "3      4  Pengukuran Website Quality Pada Situs Sistem A...   \n",
       "4      5  PENGARUH KEPEMIMPINAN DAN KOMPENSASI TERHADAP ...   \n",
       "\n",
       "                                    abstract_cleaned  \n",
       "0  abstrak satiyah pengaruh faktorfaktor pelatiha...  \n",
       "1  tujuan penelitian persepsi brand association p...  \n",
       "2                                                NaN  \n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...  \n",
       "4  abstrak penelitian metode kuantitatif menekank...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# membaca data\n",
    "df=pd.read_csv('./dataAfterPreprocessing.csv')\n",
    "# menampilkan data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee7239",
   "metadata": {},
   "source": [
    "Menghapus kolom id dan title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d4ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus data index dan title karena tidak digunakan\n",
    "df.drop(['index'],axis=1,inplace=True)\n",
    "df.drop(['title'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e7885",
   "metadata": {},
   "source": [
    "Data abstract siap digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4b98c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstrak satiyah pengaruh faktorfaktor pelatiha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tujuan penelitian persepsi brand association p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abstrak penelitian metode kuantitatif menekank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abstrak aththaariq pengaruh kompetensi dosen k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abstrak haryono arifin pengaruh perilaku konsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abstrak dharma abidin syahkesimpulan pengaruh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abstrak tujuan penelitian mengidentifikasi var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hasil penelitian perhitungan credit risk ratio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    abstract_cleaned\n",
       "0  abstrak satiyah pengaruh faktorfaktor pelatiha...\n",
       "1  tujuan penelitian persepsi brand association p...\n",
       "2                                                NaN\n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...\n",
       "4  abstrak penelitian metode kuantitatif menekank...\n",
       "5  abstrak aththaariq pengaruh kompetensi dosen k...\n",
       "6  abstrak haryono arifin pengaruh perilaku konsu...\n",
       "7  abstrak dharma abidin syahkesimpulan pengaruh ...\n",
       "8  abstrak tujuan penelitian mengidentifikasi var...\n",
       "9  hasil penelitian perhitungan credit risk ratio..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan 10 baris data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ec8fd",
   "metadata": {},
   "source": [
    "### Extracting Feature dan Membuat Document Term-Matrix (DTM)\n",
    "Nilai DTM menggunakan nilai TF-Idf.\n",
    "Beberapa poin penting yang perlu diperhatikan:\n",
    "1. LSA pada umumnya diimplementasikan dengan menggunakan nilai TF-Idf dan tidak dengan Count Vectorizer.\n",
    "2. Nilai parameter max_feature bergantung pada daya komputasi.\n",
    "3. Nilai default untuk min_df dan max_df agar program dapat bekerja dengan baik.\n",
    "4. Bisa menggunakan nilai ngram_range yang berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb9954",
   "metadata": {},
   "source": [
    "## Menghitung Tf-Idf\n",
    "Term Frequency — Inverse Document Frequency atau TF — IDF adalah suatu metode algoritma yang berguna untuk menghitung bobot setiap kata yang umum digunakan. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Metode ini akan menghitung nilai Term Frequency (TF) dan Inverse Document Frequency (IDF) pada setiap token (kata) di setiap dokumen dalam korpus. Secara sederhana, metode TF-IDF digunakan untuk mengetahui berapa sering suatu kata muncul di dalam dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27592dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(df['abstract_cleaned'].values.astype('U'))\n",
    "type(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8ac7d",
   "metadata": {},
   "source": [
    "### Document Term Matrix (DTM)\n",
    "Setiap baris mewakili sebuah kata yang unik, sedangkan setiap kolom mewakili konteks dari mana kata-kata tersebut diambil. Konteks yang dimaksud bisa berupa kalimat, paragraf, atau seluruh bagian dari teks.\n",
    "Berikut adalah term-document matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c3961",
   "metadata": {},
   "source": [
    "![Term Document Matrix](termDocumentMatrix.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4197dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 1000)\n",
      "       0     1     2     3         4         5         6         7     \\\n",
      "0  0.000000   0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000   0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.022637   0.0   0.0   0.0  0.046194  0.022923  0.029027  0.022492   \n",
      "3  0.000000   0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000   0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       8     9     ...      1021  1022      1023  1024  1025      1026  \\\n",
      "0  0.000000   0.0  ...  0.000000   0.0  0.000000   0.0   0.0  0.000000   \n",
      "1  0.000000   0.0  ...  0.000000   0.0  0.000000   0.0   0.0  0.000000   \n",
      "2  0.042889   0.0  ...  0.068666   0.0  0.068455   0.0   0.0  0.048288   \n",
      "3  0.000000   0.0  ...  0.000000   0.0  0.000000   0.0   0.0  0.000000   \n",
      "4  0.099491   0.0  ...  0.000000   0.0  0.000000   0.0   0.0  0.000000   \n",
      "\n",
      "       1027      1028      1029  1030  \n",
      "0  0.000000  0.000000  0.000000   0.0  \n",
      "1  0.000000  0.000000  0.000000   0.0  \n",
      "2  0.022502  0.031390  0.033226   0.0  \n",
      "3  0.000000  0.000000  0.000000   0.0  \n",
      "4  0.000000  0.072816  0.000000   0.0  \n",
      "\n",
      "[5 rows x 1031 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vect_text.shape)\n",
    "# print(vect_text)\n",
    "type(vect_text)\n",
    "vect_text = vect_text.transpose()\n",
    "df = pd.DataFrame(vect_text.toarray())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44fc7f",
   "metadata": {},
   "source": [
    "Kita sekarang dapat melihat kata-kata yang paling sering dan langka di abstrak berdasarkan skor idf. Semakin kecil nilainya berarti kata tersebut lebih sering digunakan (umum) dalam abstrak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91edb428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penelitian need\n",
      "1.0116960397631913\n",
      "7.246106765481563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "idf=vect.idf_\n",
    "dd=dict(zip(vect.get_feature_names(), idf))\n",
    "l=sorted(dd, key=(dd).get)\n",
    "# print(l)\n",
    "print(l[0],l[-1])\n",
    "print(dd['penelitian'])\n",
    "print(dd['need'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd74d2",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "LSA pada dasarnya adalah dekomposisi dari nilai tunggal.\n",
    "Singular Value Decomposition (SVD) akan menguraikan DTM menjadi tiga matriks: \n",
    "\n",
    "$A_{m n}=U_{m m} x S_{m n} x V_{n n}^{T}$\n",
    "\n",
    "\n",
    "Matriks U = Pada matriks ini, baris mewakili vektor dokumen pada topik\n",
    "Matriks V = Baris pada matriks ini mewakili vektor istilah yang dinyatakan pada topik\n",
    "Matriks S = Matriks diagonal yang memiliki elemen-elemen diagonal sebagai nilai singular dari A\n",
    "\n",
    "Pada setiap baris dari matriks U (matriks istilah dari dokumen) merupakan representasi vektor yang ada dalam dokumen yang sesuai. Panjang vektor ini ialah jumlah topik yang diinginkan. Representasi dari vektor untuk suku yang ada dalam data dapat ditemui dalam matriks V.\n",
    "\n",
    "Jadi, SVD memberikan nilai vektor pada setiap dokumen dan juga istilah dalam data. Panjang dari setiap vektor adalah k. Vektor ini digunakan untuk menentukan kata dan dokumen serupa dalam metode kesamaan kosinus.\n",
    "\n",
    "Dapat digunakan fungsi truncastedSVD untuk mengimplementasikan LSA. Parameter n_components merupakan jumlah topik yang akan diekstrak. Model tersebut nantinya akan di fit dan ditransformasikan pada hasil yang diberikan oleh vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3337483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1aeeef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.89289423e-02  2.86901312e-02 -1.04188392e-02 ...  1.45148962e-03\n",
      "   1.00820245e-02 -1.40954665e-03]\n",
      " [ 4.27108984e-02 -8.21232953e-02 -3.70723451e-02 ... -1.09118577e-02\n",
      "  -3.82405166e-02  9.57272418e-03]\n",
      " [ 3.03762592e-01 -1.72955836e-02  5.94445011e-02 ... -2.44425136e-02\n",
      "  -2.84279200e-02  2.78626921e-04]\n",
      " ...\n",
      " [ 3.17961147e-02 -4.36386934e-02 -2.10277937e-02 ...  8.21787841e-02\n",
      "  -1.46722190e-02  4.96628828e-03]\n",
      " [ 8.04033500e-02 -1.26289575e-01 -7.70133026e-02 ...  1.51255964e-01\n",
      "   9.48108784e-04  7.85797713e-02]\n",
      " [ 6.22278873e-02 -9.81286515e-02 -5.29993734e-02 ...  3.44695293e-02\n",
      "  -6.35377807e-03  1.39793638e-02]]\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)  # (no_of_doc*no_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05c5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  3.8928942293932054\n",
      "Topic  1  :  2.869013123944192\n",
      "Topic  2  :  -1.0418839208788628\n",
      "Topic  3  :  0.13286687248624418\n",
      "Topic  4  :  -2.7304261127065224\n",
      "Topic  5  :  0.8484590863536814\n",
      "Topic  6  :  -0.6963703409502053\n",
      "Topic  7  :  0.1451489618743057\n",
      "Topic  8  :  1.0082024523079496\n",
      "Topic  9  :  -0.1409546645626048\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "    print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c70c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1031)\n",
      "[[ 2.97801390e-02  1.04049112e-02  4.35031111e-23 ...  2.55762461e-02\n",
      "   1.69025127e-02  3.91454406e-02]\n",
      " [ 2.19071273e-02 -5.92238152e-03 -3.20285838e-20 ... -5.43556860e-03\n",
      "  -1.17288858e-02  3.96452820e-02]\n",
      " [-8.08926973e-04  2.12467817e-02 -2.40589307e-19 ...  1.62216896e-02\n",
      "   5.02803973e-02 -9.47555941e-03]\n",
      " ...\n",
      " [-2.80999914e-02  5.20044598e-03 -9.04741262e-17 ... -1.00759580e-02\n",
      "   4.53475364e-03 -1.24845300e-03]\n",
      " [-9.26909807e-02  1.40045339e-02  3.44300877e-17 ...  2.30552094e-02\n",
      "  -3.70060684e-02 -8.47409838e-02]\n",
      " [ 1.37185190e-02  4.07479013e-02 -4.24922681e-17 ... -7.58364019e-03\n",
      "  -6.34430579e-02  2.96579593e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fcc31",
   "metadata": {},
   "source": [
    "### Hasil\n",
    "Berikut adalah 10 kata penting dalam setiap topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc63e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "layanan menemukan wawancara mmt menyebabkan kompensasi pranjoto share populasinya pimpinan \n",
      "\n",
      "Topic 1: \n",
      "share menemukan populasinya memperhatikan hotel klasik tabel motivasi pemimpin st \n",
      "\n",
      "Topic 2: \n",
      "standar kesejahteraan atribut badan properti asosiasi financial dinamika dibandingkan eva \n",
      "\n",
      "Topic 3: \n",
      "puskesmas pasar supervisi pendapatan transportasi panca probability pamekasan kerjasama penyebaran \n",
      "\n",
      "Topic 4: \n",
      "beli kinerja menciptakan merger indeks konsumsi pemimpin perputaran dibawah mengambil \n",
      "\n",
      "Topic 5: \n",
      "satuan anggota assurance payout instagram tabungan harapan tingkat group orang \n",
      "\n",
      "Topic 6: \n",
      "banyaknya kerja diukur deposit adiluhung kehandalan terikatnya chrismardani efek menghasilkan \n",
      "\n",
      "Topic 7: \n",
      "iriani efek diolah kuswinarno isi observasi mitra transformasional efisien bersaing \n",
      "\n",
      "Topic 8: \n",
      "memperhatikan kerja adiluhung dana diukur merek sub customer prasetyo kpri \n",
      "\n",
      "Topic 9: \n",
      "cash negara kabupaten intrinsik bahan karakteristik penarikan apt dihitung indosat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce095f",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "K-means merupakan salah satu algoritma yang bersifat unsupervised learning. K-Means memiliki fungsi untuk mengelompokkan data kedalam data cluster. Algoritma ini dapat menerima data tanpa ada label kategori. K-Means Clustering Algoritma juga merupakan metode non-hierarchy. Metode Clustering Algoritma adalah mengelompokkan beberapa data ke dalam kelompok yang menjelaskan data dalam satu kelompok memiliki karakteristik yang sama dan memiliki karakteristik yang berbeda dengan data yang ada di kelompok lain. Cluster Sampling adalah teknik pengambilan sampel di mana unit-unit populasi dipilih secara acak dari kelompok yang sudah ada yang disebut ‘cluster, nah Clustering atau klasterisasi adalah salah satu masalah yang menggunakan teknik unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5bf26",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a167936",
   "metadata": {},
   "source": [
    "### Membuat Model K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(vect_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f6cf4",
   "metadata": {},
   "source": [
    "### Hasil\n",
    "Menampilkan hasil K-means clustering dengan 5 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vect.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672e980",
   "metadata": {},
   "source": [
    "### code lengkap Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d18f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " bank\n",
      " tbk\n",
      " pt\n",
      " keuangan\n",
      " rasio\n",
      " eva\n",
      " portofolio\n",
      " perusahaan\n",
      " saham\n",
      " value\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      " keputusan\n",
      " pembelian\n",
      " produk\n",
      " merek\n",
      " konsumen\n",
      " variabel\n",
      " uji\n",
      " promosi\n",
      " penelitian\n",
      " kualitas\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      " kinerja\n",
      " variabel\n",
      " penelitian\n",
      " kepuasan\n",
      " pegawai\n",
      " berpengaruh\n",
      " kerja\n",
      " signifikan\n",
      " pengaruh\n",
      " bangkalan\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      " perusahaan\n",
      " ratio\n",
      " saham\n",
      " return\n",
      " penelitian\n",
      " bursa\n",
      " efek\n",
      " terdaftar\n",
      " profitabilitas\n",
      " indonesia\n",
      "\n",
      "\n",
      "Cluster 4:\n",
      " kerja\n",
      " karyawan\n",
      " kinerja\n",
      " variabel\n",
      " motivasi\n",
      " kepuasan\n",
      " berpengaruh\n",
      " disiplin\n",
      " nilai\n",
      " positif\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(vect_text)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vect.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}